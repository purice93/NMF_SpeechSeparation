.LOG
2018/1/15
语音分离


我的一些思考：
	我们当前研究的是单通道语音分离，即我们认为人是可以区分出语音的。
	但是问题是，人有两个耳朵，即双通道，所以如果是单通道的，人类是否也能区分出来？
	
一、研究背景-1953年鸡尾酒会问题
	1、听觉辅助
	2、移动通信（手机手环等）
	3、自动演讲
	4、语音识别前端

二、基本理论：
	语音分离的三个方面：
		1、语音增强：即语音和非语音
		2、多说话人语音分离：语音-语音分离
		3、解混响：自己的语音和自己的语音（回音）
	分类：
		1、单通道多说话人
			传统方式：语音增强和计算机听觉场景分析（speech enhancement;computer auditory scene analysis-CASA）
				语音增强：基于语音和噪音的常规统计
					最长用的方法：谱减法-spectral subtraction
						即混合语音减去噪音的能量谱：
							但是为了估算背景噪声，所以默认噪声的光谱信息是稳定不变的，不随时间变化
							或者说至少要比纯语音信息稳定
				CASA:这个是基于场景尽心计算机模拟分析，需要消耗大量的人力物力
				
		2、多麦克风阵列

三、当前现状改变
	以往的方法主要使用的信号处理的方式，属于无监督的学习过程
	最近的几年开始将语音分离作为一种监督的学习问题
	最初的监督学习主要来源于基于CASA的时频掩蔽方式，CASA的主要目标是ideal binary mask (IBM)
	
深度学习的方式步骤：
	1、学习机：深度学习，卷积等
	2、目标函数构造
	3、特征提取（声学特征）

单通道的语音分离有一些传统的方法像谱减法 
还有就是近几年比较火的基于深度学习(dnn)的方法 
基于dnn的方法主要:
	1、mapping就是直接的谱映射的方法
	2、基于掩蔽的方法 如IBM IRM等 
	3、之前还有看到一些文章是基于多目标的方法 
		推荐你读一下汪德亮2017年的一篇总结的文章
		《an overview supervised speech separation based on deep learning》
		
		
		
		
关键词解析：
1、IBM-理想二值掩蔽
	“理想二值掩蔽”（Ideal Binary Mask）中的分离任务就成为了一个二分类问题。
	这类方法根据听觉感知特性，把音频信号分成不同的子带，根据每个时频单元上的信噪比，
	把对应的时频单元的能量设为0（噪音占主导的情况下）或者保持原样（目标语音占主导的情况下）。
	
	
	
	
	